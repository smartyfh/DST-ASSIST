# SOM-DST

The SOM-DST model is an open vocabulary-based approach. We took SOM-DST as one primary model to test if the generated pseudo labels can also help train models that generate the states directly.

## Usage

### Data Preprocessing

There are two steps that preprocess the dataset to the format required by the model.

```console
❱❱❱ python3 create_data.py --mwz_ver 2.0
❱❱❱ python3 preprocess_data.py --data_dir data/mwz2.0
```

or

```console
❱❱❱ python3 create_data.py --mwz_ver 2.4
❱❱❱ python3 preprocess_data.py --data_dir data/mwz2.4
```

### Training

Before training the model, we need to copy the generated pseudo labels by the AUX-DST model to the current directory at first. Then simply run:

```console
❱❱❱ python3 train-pseudo.py --data_root data/mwz2.0 --save_dir output-pseudo-20/exp --alpha 0.6
```

or


```console
❱❱❱ python3 train-pseudo.py --data_root data/mwz2.4 --save_dir output-pseudo-24/exp --alpha 0.4
```

**Note:** The parameter *alpha* is utilized to adjust the weights of the pseudo labels and the vanilla noisy labels. When *alpha=1*, only the pseudo labels are effective. When "alpha=0", only the vanilla labels are effective.

## Links

* The pseudp labels generated by AUX-DST can be downloaded [here](https://drive.google.com/file/d/1xrzhbEIou7h-qS1yRd83vKVnR6ZGmotp/view?usp=sharing).
